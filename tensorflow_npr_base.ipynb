{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451002ed-58a8-466c-9123-de151a607d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 10:49:52.772034: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a85639-dd9b-43cc-8205-a9e5cfc712d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = pd.read_csv('SH3_Full_Dataset_8_9_22.csv')\n",
    "msa['Type'].unique()\n",
    "naturals_msa = msa[msa['Type']=='Naturals']\n",
    "seqs = np.asarray([list(seq) for seq in naturals_msa['Sequences']])\n",
    "norm_re = np.asarray([re for re in naturals_msa['Norm_RE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a89a9d-8957-4b35-962b-e3b5e494eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_aa_keys='-GALMFWKQESPVICYHRNDT'\n",
    "def fasta_to_df(fasta_file, aa_keys = default_aa_keys):\n",
    "    \"\"\"\n",
    "    creates one hot encoding of a fasta file using biopython's alignio.read process. \n",
    "    fasta_file : filepath leading to msa file in fasta format at hand\n",
    "    \"\"\"\n",
    "    column_names = []\n",
    "    column_names.extend(aa_keys)\n",
    "    msa=AlignIO.read(fasta_file, \"fasta\")\n",
    "    num_columns = len(msa[0].seq)\n",
    "    column_names = column_names*num_columns\n",
    "    column_names.append('sequence')\n",
    "    column_names.append('id')\n",
    "    init = np.zeros((len(msa), len(column_names)))\n",
    "    df = pd.DataFrame(init, columns = column_names)\n",
    "    df.sequence = df.sequence.astype(str)\n",
    "    df.id=df.id.astype(str)\n",
    "    \n",
    "    for row_num, alignment in tqdm(enumerate(msa)):\n",
    "        sequence = str(alignment.seq)\n",
    "        for index, char in enumerate(sequence):\n",
    "            place = aa_keys.find(char)\n",
    "            df.iloc[row_num, index*len(aa_keys) + place] = 1\n",
    "        \n",
    "        df.iloc[row_num,-2]=str(alignment.seq)\n",
    "        df.iloc[row_num,-1]=str(alignment.id)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289431ac-7732-46ac-9c52-c45594e6d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_matrix(df, aa_keys = default_aa_keys):\n",
    "    \"\"\"takes one hot encoded msa and returns the frequency of each amino acid at each site\n",
    "    df : pandas dataframe whose columns are the one hot encoding of an msa\n",
    "    \"\"\"\n",
    "    num_columns=len(df['sequence'][0])\n",
    "    \n",
    "    frequency_matrix = np.zeros( (len(aa_keys) , num_columns) )\n",
    "    print('calcing sum')\n",
    "    freq=df.sum()\n",
    "    print('sum calced')\n",
    "    \n",
    "    num_entries=len(df)\n",
    "    len_aa_keys = len(aa_keys)\n",
    "    \n",
    "    for i in tqdm(range(len(aa_keys))):\n",
    "        for j in range(num_columns):\n",
    "            frequency_matrix[i, j] = freq[ i + len_aa_keys * j] / num_entries\n",
    "    \n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2961ba-d77d-4537-85dd-1a5ceb3fa740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11608it [01:58, 97.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcing sum\n",
      "sum calced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 10633.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 44]\n"
     ]
    }
   ],
   "source": [
    "from Bio import AlignIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from tqdm import tqdm\n",
    "vae_alignment = []\n",
    "phenotypes = []\n",
    "\n",
    "vae_data = msa[msa['Type']=='VAE'].reset_index()\n",
    "\n",
    "for r in range(len(vae_data)):\n",
    "    alignment = vae_data.loc[r]\n",
    "    if len(alignment['Sequences'])==62:\n",
    "        record = SeqRecord(seq = Seq(alignment['Sequences']), id = alignment['Header'])\n",
    "    \n",
    "    vae_alignment.append(record)\n",
    "    phenotypes.append(alignment['Norm_RE'])\n",
    "\n",
    "vae_alignment = AlignIO.MultipleSeqAlignment(vae_alignment)\n",
    "\n",
    "AlignIO.write(vae_alignment, 'vae_alignment.fasta', 'fasta')\n",
    "\n",
    "vae_df = fasta_to_df('vae_alignment.fasta')\n",
    "\n",
    "freq_matrix = create_frequency_matrix(vae_df)\n",
    "\n",
    "trim_positions = []\n",
    "\n",
    "for i in range(freq_matrix.shape[1]):\n",
    "    if 1 in freq_matrix[:,i]:\n",
    "        trim_positions.append(i)\n",
    "\n",
    "print(trim_positions)\n",
    "\n",
    "\n",
    "vae_alignment_trimmed = []\n",
    "\n",
    "\n",
    "for alignment in vae_alignment:\n",
    "    new_seq = ''\n",
    "    for i in range(62):\n",
    "        if i not in trim_positions:\n",
    "            new_seq+=alignment.seq[i]\n",
    "    re_alignment = SeqRecord(seq=Seq(new_seq), id = alignment.id)\n",
    "    vae_alignment_trimmed.append(re_alignment)\n",
    "\n",
    "vae_alignment_trimmed = AlignIO.MultipleSeqAlignment(vae_alignment_trimmed)\n",
    "\n",
    "AlignIO.write(vae_alignment_trimmed, 'vae_alignment_trimmed.fasta', 'fasta')\n",
    "\n",
    "test_seqs = np.asarray([list(str(alignment.seq)) for alignment in vae_alignment_trimmed])\n",
    "\n",
    "phenotypes = np.asarray(phenotypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5bf509-2d6b-499d-bed3-94d48252956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMINO_ACIDS = \"ARNDCQEGHILKMFPSTWYV-\"\n",
    "IDX_TO_AA = list(AMINO_ACIDS)\n",
    "AA_TO_IDX = {aa: i for i, aa in enumerate(IDX_TO_AA)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775ea6eb-b66a-4c80-a4fd-ef806ff2c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNPRegressionDescription = collections.namedtuple(\n",
    "    \"CNPRegressionDescription\",\n",
    "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621a2c3d-6620-4619-aff4-29a6afa7c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Optional[np.ndarray] = None,\n",
    "        top_model = False,\n",
    "        regression_target = None,\n",
    "        batch_size: int = 100,\n",
    "        n_steps_per_epoch: int = 50,\n",
    "        shuffle: bool = True\n",
    "    ):\n",
    "\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps_per_epoch = n_steps_per_epoch\n",
    "        self.shuffle = shuffle\n",
    "        self.top_model = top_model\n",
    "        \n",
    "        if self.shuffle:\n",
    "            self.step_batch_idx = np.random.choice(len(self.data), size=(self.n_steps_per_epoch, self.batch_size))\n",
    "        else:\n",
    "            self.step_batch_idx = np.arange(len(self.data)).reshape((1, -1))\n",
    "        \n",
    "        if self.top_model:\n",
    "            self.regression_target = regression_target\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n_steps_per_epoch\n",
    "        \n",
    "    def __getitem__(self, i_batch):\n",
    "        output = self.__data_generation(i_batch)\n",
    "        X, y = output.query, output.target_y\n",
    "            \n",
    "        return X, y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "\n",
    "            self.step_batch_idx = np.random.choice(len(self.data), size=(self.n_steps_per_epoch, self.batch_size))\n",
    "        else:\n",
    "            self.step_batch_idx = np.arange(len(self.data)).reshape((1, -1))\n",
    "        \n",
    "    def __data_generation(self, i_batch):\n",
    "        batch_idx = self.step_batch_idx[i_batch]\n",
    "\n",
    "        \n",
    "        num_context_points = tf.random.uniform(shape=[], minval=3, maxval=int(0.9*self.data.shape[1]), dtype=tf.int32)\n",
    "        num_target_points = self.data.shape[1] - num_context_points\n",
    "\n",
    "        context_X = np.zeros((self.batch_size, num_context_points, 1), dtype = np.float32)\n",
    "        context_Y = np.zeros((self.batch_size, num_context_points, len(AA_TO_IDX)), dtype = np.float32)\n",
    "\n",
    "        target_X = np.zeros((self.batch_size, num_target_points, 1), dtype=np.float32)\n",
    "        target_Y = np.zeros((self.batch_size, num_target_points, len(AA_TO_IDX)), dtype = np.float32)\n",
    "        \n",
    "        for i_data, idx in enumerate(batch_idx):\n",
    "            \n",
    "            shuffled_indices = tf.random.shuffle(tf.range(self.data.shape[1]))\n",
    "            context_X[i_data,:,0] = shuffled_indices[:num_context_points]\n",
    "            target_X[i_data, :,0] = shuffled_indices[num_context_points:]\n",
    "\n",
    "            for y_index, context_index in enumerate(context_X[i_data,:]):\n",
    "                aa = self.data[idx, int(context_index)]\n",
    "                context_Y[i_data, y_index, AA_TO_IDX[aa]] = 1\n",
    "\n",
    "                    \n",
    "            for y_index, target_index in enumerate(target_X[i_data,:]):\n",
    "                aa = self.data[idx, int(target_index)]\n",
    "                target_Y[i_data, y_index, AA_TO_IDX[aa]] = 1\n",
    "            \n",
    "        query = ((context_X, context_Y), target_X)\n",
    "\n",
    "        return CNPRegressionDescription(\n",
    "            query = query,\n",
    "            target_y = target_Y,\n",
    "            num_total_points = self.data.shape[1]-num_context_points,\n",
    "            num_context_points = num_context_points)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a96c37c-9799-45c7-a3c2-b14b991448f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, _, _ = train_test_split(seqs, norm_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d1189d-b186-4bf6-bfc7-667204cd0a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5898, 59)\n",
      "(1967, 59)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d3238e9-2899-4eb2-b66f-174d40e6a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_npr = DataGenerator(\n",
    "    data = X_train\n",
    ")\n",
    "\n",
    "dg_npr_val = DataGenerator(\n",
    "    data = X_test,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "dg_npr_test = DataGenerator(\n",
    "    data = test_seqs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc452734-7813-426f-8417-6400567fd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicEncoderProtein(tf.keras.Model):\n",
    "    def __init__(self, output_sizes, **kwargs):\n",
    "        super(DeterministicEncoderProtein, self).__init__(**kwargs)\n",
    "        self._output_sizes = output_sizes\n",
    "        self.mlp = tf.keras.models.Sequential()\n",
    "        for i, size in enumerate(output_sizes[:-1]):\n",
    "            self.mlp.add(tf.keras.layers.Dense(size, activation='relu'))\n",
    "        self.mlp.add(tf.keras.layers.Dense(output_sizes[-1]))\n",
    "        \n",
    "\n",
    "    def call(self, context_x, context_y, num_context_points):\n",
    "        encoder_input = tf.concat([context_x, context_y], axis = -1)\n",
    "\n",
    "        batch_size, _, filter_size = encoder_input.shape.as_list()\n",
    "        hidden = tf.reshape(encoder_input, (batch_size * num_context_points, -1))\n",
    "        hidden.set_shape((None, filter_size))\n",
    "        hidden = self.mlp(hidden)\n",
    "\n",
    "        hidden = tf.reshape(hidden, (batch_size, num_context_points, self._output_sizes[-1]))\n",
    "        representation = tf.reduce_mean(hidden, axis=1)\n",
    "\n",
    "        return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0781db1-9476-4948-9bb0-30637061c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicDecoderProtein(tf.keras.Model):\n",
    "    def __init__(self, output_sizes, **kwargs):\n",
    "        super(DeterministicDecoderProtein, self).__init__(**kwargs)\n",
    "        self._output_sizes = output_sizes\n",
    "        self.mlp = tf.keras.models.Sequential()\n",
    "        for i, size in enumerate(output_sizes[:-1]):\n",
    "            self.mlp.add(tf.keras.layers.Dense(size, activation='relu'))\n",
    "        self.mlp.add(tf.keras.layers.Dense(output_sizes[-1]))\n",
    "        self.mlp.add(tf.keras.layers.Softmax())\n",
    "\n",
    "    def call(self, representation, target_x, num_total_points):\n",
    "        # Concatenate the representation and the target_x\n",
    "        representation = tf.tile(\n",
    "            tf.expand_dims(representation, axis=1), [1, num_total_points, 1])\n",
    "        input = tf.concat([representation, target_x], axis=-1)\n",
    "        \n",
    "        # Get the shapes of the input and reshape to parallelise across observations\n",
    "        batch_size, _, filter_size = input.shape.as_list()\n",
    "        hidden = tf.reshape(input, (batch_size * num_total_points, -1))\n",
    "        hidden.set_shape((None, filter_size))\n",
    "\n",
    "        hidden = self.mlp(hidden)\n",
    "\n",
    "        hidden = tf.reshape(hidden, (batch_size, num_total_points, -1))\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91090b67-9f51-4b70-b31a-c7bd9a0889b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DeterministicEncoderProtein([128,128,128,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02af85c-ce02-436d-bc8d-fe189d441baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DeterministicDecoderProtein([128,128,128,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb8b9fc-9a29-46c8-8ba3-49c0f65122d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = dg_npr[0]\n",
    "X = test[0]\n",
    "y = test[1]\n",
    "\n",
    "context_X = X[0][0]\n",
    "context_y = X[0][1]\n",
    "target_X = X[1]\n",
    "target_y = y\n",
    "num_context_points = context_X.shape[1]\n",
    "num_target_points = target_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3531d624-b750-43ad-9ce2-79cb88f683ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 36, 1)\n",
      "(100, 36, 21)\n",
      "(100, 23, 1)\n",
      "(100, 23, 21)\n"
     ]
    }
   ],
   "source": [
    "print(context_X.shape)\n",
    "print(context_y.shape)\n",
    "print(target_X.shape)\n",
    "print(target_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac63493-3c78-4823-a951-5a6f6b26df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = encoder(context_X, context_y, num_context_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "359528c3-6bb7-4a2c-bcf2-1a4d5433e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = decoder(rep, target_X, num_target_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35ff6059-53c4-42ec-8960-f0dded80cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    eps=1e-6\n",
    "    mean_cross_ent_per_pos = -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred+eps), axis=-1))\n",
    "    return mean_cross_ent_per_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb973a61-43bf-4b53-a933-0af5e554dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicModel(tf.keras.Model):\n",
    "    \"\"\"The CNP model.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_output_sizes, decoder_output_sizes, **kwargs):\n",
    "        \"\"\"Initialises the model.\n",
    "    \n",
    "        Args:\n",
    "          encoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
    "              the encoder. The last one is the size of the representation r.\n",
    "          decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
    "              the decoder. The last element should correspond to the dimension of\n",
    "              the y * 2 (it encodes both mean and variance concatenated)\n",
    "        \"\"\"\n",
    "        super(DeterministicModel, self).__init__(**kwargs)\n",
    "        self.encoder = DeterministicEncoderProtein(encoder_output_sizes)\n",
    "        self.decoder = DeterministicDecoderProtein(decoder_output_sizes)\n",
    "        \n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name='total_loss')\n",
    "        self.val_total_loss_tracker = tf.keras.metrics.Mean(name='val_total_loss')\n",
    "        \n",
    "        \n",
    "\n",
    "    def call(self, data):\n",
    "        query, target_y = data\n",
    "        num_total_points = target_y.shape[1]\n",
    "        num_context_points = query[0][0].shape[1]\n",
    "\n",
    "        representation = self.encoder(query[0][0], query[0][1], num_context_points)\n",
    "        decoded_output = self.decoder(representation, query[1], num_total_points)\n",
    "\n",
    "        return decoded_output\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "        ]\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            query, target_y = data\n",
    "            num_total_points = target_y.shape[1]\n",
    "            num_context_points = query[0][0].shape[1]\n",
    "            representation = self.encoder(query[0][0], query[0][1], num_context_points)\n",
    "            decoded_output = self.decoder(representation, query[1], num_total_points)\n",
    "\n",
    "\n",
    "            loss = cross_entropy_loss(target_y, decoded_output)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(loss)\n",
    "\n",
    "        return {'total_loss':self.total_loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        query, target_y = data\n",
    "        num_total_points = target_y.shape[1]\n",
    "        num_context_points = query[0][0].shape[1]\n",
    "        \n",
    "        representation = self.encoder(query[0][0], query[0][1], num_context_points)\n",
    "        decoded_output = self.decoder(representation, query[1], num_total_points)\n",
    "\n",
    "\n",
    "\n",
    "        loss = cross_entropy_loss(target_y, decoded_output)\n",
    "        self.val_total_loss_tracker.update_state(loss)\n",
    "\n",
    "        return {\n",
    "            'total_loss' : self.val_total_loss_tracker.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6dba7e33-ebec-44a2-9555-f8bcc2988135",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr_model = DeterministicModel([128,128,128,128],[128,128,128,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "137e9241-a887-43c8-b54b-75f95312bad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 10, 21), dtype=float32, numpy=\n",
       "array([[[5.12223132e-02, 1.19180400e-02, 4.59415745e-03, ...,\n",
       "         6.69588074e-02, 6.32622913e-02, 1.25995036e-02],\n",
       "        [3.08917500e-02, 2.44209711e-02, 2.11646184e-02, ...,\n",
       "         7.60855153e-02, 6.14675507e-02, 4.49326672e-02],\n",
       "        [3.04797739e-02, 9.63259139e-04, 1.90347811e-04, ...,\n",
       "         3.32372189e-02, 4.62113880e-02, 8.23027105e-04],\n",
       "        ...,\n",
       "        [3.15168910e-02, 2.54451856e-02, 1.94364451e-02, ...,\n",
       "         8.11143070e-02, 5.84905557e-02, 4.15690504e-02],\n",
       "        [4.66504022e-02, 1.43546462e-02, 6.34056050e-03, ...,\n",
       "         6.45800158e-02, 6.68065175e-02, 1.56924687e-02],\n",
       "        [3.18955779e-02, 2.45730579e-02, 2.14529969e-02, ...,\n",
       "         7.28097558e-02, 6.13613762e-02, 4.56692688e-02]],\n",
       "\n",
       "       [[4.72401269e-02, 3.55296093e-03, 9.46376531e-04, ...,\n",
       "         5.39648198e-02, 5.53080738e-02, 3.35250958e-03],\n",
       "        [4.10993136e-02, 2.06088088e-03, 4.76720801e-04, ...,\n",
       "         4.43007238e-02, 5.12977205e-02, 1.83500547e-03],\n",
       "        [5.14096096e-02, 1.11084059e-02, 4.07473836e-03, ...,\n",
       "         6.71928748e-02, 6.28282577e-02, 1.16641484e-02],\n",
       "        ...,\n",
       "        [3.08929421e-02, 2.50744298e-02, 1.89307109e-02, ...,\n",
       "         8.19437876e-02, 5.87164722e-02, 4.15295139e-02],\n",
       "        [3.25602256e-02, 2.42940113e-02, 1.80209484e-02, ...,\n",
       "         8.32671821e-02, 5.77699915e-02, 4.02092040e-02],\n",
       "        [5.35003506e-02, 9.64765716e-03, 3.33241257e-03, ...,\n",
       "         6.78801835e-02, 6.10060990e-02, 9.95492749e-03]],\n",
       "\n",
       "       [[3.17925662e-02, 2.58254055e-02, 1.98042672e-02, ...,\n",
       "         8.05617422e-02, 5.83996661e-02, 4.17757221e-02],\n",
       "        [3.26053537e-02, 1.17656041e-03, 2.44450610e-04, ...,\n",
       "         3.54057252e-02, 4.78354357e-02, 1.00925646e-03],\n",
       "        [3.93737629e-02, 1.92217529e-02, 1.18695460e-02, ...,\n",
       "         7.01682195e-02, 6.78590834e-02, 2.66998578e-02],\n",
       "        ...,\n",
       "        [5.28942272e-02, 1.12645160e-02, 4.21830220e-03, ...,\n",
       "         6.74697459e-02, 6.21801168e-02, 1.17213428e-02],\n",
       "        [3.80559713e-02, 1.72560150e-03, 3.89469700e-04, ...,\n",
       "         4.06145900e-02, 5.05151972e-02, 1.49972772e-03],\n",
       "        [3.00611872e-02, 9.67551139e-04, 1.92853433e-04, ...,\n",
       "         3.29218544e-02, 4.63835150e-02, 8.24517629e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3.89853455e-02, 1.99759901e-02, 1.29545005e-02, ...,\n",
       "         7.30393603e-02, 6.53188080e-02, 2.97137611e-02],\n",
       "        [3.29198055e-02, 1.17531989e-03, 2.42025708e-04, ...,\n",
       "         3.54840904e-02, 4.75999974e-02, 1.00945751e-03],\n",
       "        [3.16887163e-02, 2.57468875e-02, 1.95322260e-02, ...,\n",
       "         8.07239786e-02, 5.82420826e-02, 4.17726189e-02],\n",
       "        ...,\n",
       "        [4.99647222e-02, 4.62519890e-03, 1.35585642e-03, ...,\n",
       "         5.77903576e-02, 5.71098775e-02, 4.47539752e-03],\n",
       "        [5.38702123e-02, 7.00343726e-03, 2.32319115e-03, ...,\n",
       "         6.51604831e-02, 5.86180128e-02, 7.13782292e-03],\n",
       "        [5.42465970e-02, 9.67602152e-03, 3.42089171e-03, ...,\n",
       "         6.76284283e-02, 6.06876798e-02, 9.93414968e-03]],\n",
       "\n",
       "       [[1.95344146e-02, 3.59340629e-04, 5.55674851e-05, ...,\n",
       "         2.16946285e-02, 3.82002667e-02, 2.82331748e-04],\n",
       "        [5.27448468e-02, 1.12359049e-02, 4.21003811e-03, ...,\n",
       "         6.74742684e-02, 6.22528605e-02, 1.17363865e-02],\n",
       "        [4.39175107e-02, 2.71451124e-03, 6.92374946e-04, ...,\n",
       "         4.78792936e-02, 5.36958054e-02, 2.46897084e-03],\n",
       "        ...,\n",
       "        [5.41222095e-02, 8.24674126e-03, 2.84467055e-03, ...,\n",
       "         6.65818453e-02, 5.99127077e-02, 8.43265746e-03],\n",
       "        [4.06749286e-02, 2.07128143e-03, 4.92469175e-04, ...,\n",
       "         4.32835855e-02, 5.16369529e-02, 1.82545895e-03],\n",
       "        [2.54525095e-02, 6.52416144e-04, 1.17673640e-04, ...,\n",
       "         2.80622356e-02, 4.30585034e-02, 5.39813540e-04]],\n",
       "\n",
       "       [[3.78691554e-02, 2.07656659e-02, 1.44026019e-02, ...,\n",
       "         7.67459869e-02, 6.34017587e-02, 3.27443480e-02],\n",
       "        [3.03053800e-02, 2.50348225e-02, 2.00629719e-02, ...,\n",
       "         8.10690224e-02, 6.00438789e-02, 4.32073921e-02],\n",
       "        [3.47314253e-02, 2.31149513e-02, 1.73201729e-02, ...,\n",
       "         8.23695511e-02, 5.95861152e-02, 3.90647948e-02],\n",
       "        ...,\n",
       "        [3.31153870e-02, 1.16634206e-03, 2.38438850e-04, ...,\n",
       "         3.60929556e-02, 4.74566296e-02, 1.00213371e-03],\n",
       "        [4.49417830e-02, 2.95611634e-03, 7.58792099e-04, ...,\n",
       "         5.04450873e-02, 5.40801287e-02, 2.72954628e-03],\n",
       "        [4.07685488e-02, 2.05456722e-03, 4.82210860e-04, ...,\n",
       "         4.41838279e-02, 5.13986796e-02, 1.82551926e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr_model(dg_npr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0652373-fcbf-4bd8-81e2-6af1905c5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a996cad3-ae44-48e1-8886-3dd8be3293ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_total_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1ca92-4d35-4f62-b2d3-ef5448dce52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 12s 239ms/step - total_loss: 2.7315 - val_total_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 11s 213ms/step - total_loss: 2.6975 - val_total_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 10s 206ms/step - total_loss: 2.6892 - val_total_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 11s 212ms/step - total_loss: 2.6627 - val_total_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 12s 232ms/step - total_loss: 2.6519 - val_total_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 11s 221ms/step - total_loss: 2.6328 - val_total_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 10s 207ms/step - total_loss: 2.6301 - val_total_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 10s 207ms/step - total_loss: 2.6126 - val_total_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 11s 213ms/step - total_loss: 2.5900 - val_total_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 10s 209ms/step - total_loss: 2.5761 - val_total_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 12s 239ms/step - total_loss: 2.5826 - val_total_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 11s 226ms/step - total_loss: 2.5595 - val_total_loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 11s 228ms/step - total_loss: 2.5415 - val_total_loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 10s 210ms/step - total_loss: 2.5365 - val_total_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 10s 210ms/step - total_loss: 2.5181 - val_total_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 11s 222ms/step - total_loss: 2.5039 - val_total_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 12s 232ms/step - total_loss: 2.5070 - val_total_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 10s 209ms/step - total_loss: 2.5034 - val_total_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 10s 206ms/step - total_loss: 2.4668 - val_total_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 10s 209ms/step - total_loss: 2.4630 - val_total_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 10s 207ms/step - total_loss: 2.4645 - val_total_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 12s 241ms/step - total_loss: 2.4653 - val_total_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 11s 220ms/step - total_loss: 2.4787 - val_total_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 10s 210ms/step - total_loss: 2.4962 - val_total_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 14s 274ms/step - total_loss: 2.4613 - val_total_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "40/50 [=======================>......] - ETA: 1s - total_loss: 2.4490"
     ]
    }
   ],
   "source": [
    "npr_model.fit(dg_npr, epochs=100, validation_data=dg_npr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca40b4-cc16-4e1c-a543-ad4d1829df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d802ffd-9c41-4dca-a16a-02b662572465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [03:07<00:00,  3.74s/it]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in tqdm(range(dg_npr_test.n_steps_per_epoch)):\n",
    "    test = dg_npr_test[i]\n",
    "    target_y = test[1]\n",
    "    decoded_probabilities = npr_model(test)\n",
    "    for j in range(dg_npr_test.batch_size):\n",
    "        for k in range(target_y.shape[1]):\n",
    "            argmax_guess = np.where(np.nanmax(decoded_probabilities[j][k])==decoded_probabilities[j][k])[0][0]\n",
    "            real = np.where(np.nanmax(target_y[j][k])==target_y[j][k])[0][0]\n",
    "            if argmax_guess == real:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "    print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1370c41f-032d-4ad4-a526-024bfeb33262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3455126050420168\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093311e-7608-4971-88ff-147077f15a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
