{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12bba58-ee1a-4465-b723-5235622f0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from NeuralProcessClasses_Proteins import *\n",
    "from architecture_classes import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3db1fcd-7707-4b05-bdf1-d980908f3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = pd.read_csv('SH3_Full_Dataset_8_9_22.csv')\n",
    "msa['Type'].unique()\n",
    "naturals_msa = msa[msa['Type']=='Naturals']\n",
    "seqs = np.asarray([list(seq) for seq in naturals_msa['Sequences']])\n",
    "norm_re = np.asarray([re for re in naturals_msa['Norm_RE']])\n",
    "\n",
    "default_aa_keys='-GALMFWKQESPVICYHRNDT'\n",
    "def fasta_to_df(fasta_file, aa_keys = default_aa_keys):\n",
    "    \"\"\"\n",
    "    creates one hot encoding of a fasta file using biopython's alignio.read process. \n",
    "    fasta_file : filepath leading to msa file in fasta format at hand\n",
    "    \"\"\"\n",
    "    column_names = []\n",
    "    column_names.extend(aa_keys)\n",
    "    msa=AlignIO.read(fasta_file, \"fasta\")\n",
    "    num_columns = len(msa[0].seq)\n",
    "    column_names = column_names*num_columns\n",
    "    column_names.append('sequence')\n",
    "    column_names.append('id')\n",
    "    init = np.zeros((len(msa), len(column_names)))\n",
    "    df = pd.DataFrame(init, columns = column_names)\n",
    "    df.sequence = df.sequence.astype(str)\n",
    "    df.id=df.id.astype(str)\n",
    "    \n",
    "    for row_num, alignment in tqdm(enumerate(msa)):\n",
    "        sequence = str(alignment.seq)\n",
    "        for index, char in enumerate(sequence):\n",
    "            place = aa_keys.find(char)\n",
    "            df.iloc[row_num, index*len(aa_keys) + place] = 1\n",
    "        \n",
    "        df.iloc[row_num,-2]=str(alignment.seq)\n",
    "        df.iloc[row_num,-1]=str(alignment.id)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b21f2cf-79ef-4424-b94c-d9924b2e04d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_matrix(df, aa_keys = default_aa_keys):\n",
    "    \"\"\"takes one hot encoded msa and returns the frequency of each amino acid at each site\n",
    "    df : pandas dataframe whose columns are the one hot encoding of an msa\n",
    "    \"\"\"\n",
    "    num_columns=len(df['sequence'][0])\n",
    "    \n",
    "    frequency_matrix = np.zeros( (len(aa_keys) , num_columns) )\n",
    "    print('calcing sum')\n",
    "    freq=df.sum()\n",
    "    print('sum calced')\n",
    "    \n",
    "    num_entries=len(df)\n",
    "    len_aa_keys = len(aa_keys)\n",
    "    \n",
    "    for i in tqdm(range(len(aa_keys))):\n",
    "        for j in range(num_columns):\n",
    "            frequency_matrix[i, j] = freq[ i + len_aa_keys * j] / num_entries\n",
    "    \n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50738df-3847-4dce-b69a-f086ef092ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11608it [02:40, 72.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcing sum\n",
      "sum calced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 21/21 [00:00<00:00, 7291.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Bio import AlignIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from tqdm import tqdm\n",
    "vae_alignment = []\n",
    "phenotypes = []\n",
    "\n",
    "vae_data = msa[msa['Type']=='VAE'].reset_index()\n",
    "\n",
    "for r in range(len(vae_data)):\n",
    "    alignment = vae_data.loc[r]\n",
    "    if len(alignment['Sequences'])==62:\n",
    "        record = SeqRecord(seq = Seq(alignment['Sequences']), id = alignment['Header'])\n",
    "    \n",
    "    vae_alignment.append(record)\n",
    "    phenotypes.append(alignment['Norm_RE'])\n",
    "\n",
    "vae_alignment = AlignIO.MultipleSeqAlignment(vae_alignment)\n",
    "\n",
    "AlignIO.write(vae_alignment, 'vae_alignment.fasta', 'fasta')\n",
    "\n",
    "vae_df = fasta_to_df('vae_alignment.fasta')\n",
    "\n",
    "freq_matrix = create_frequency_matrix(vae_df)\n",
    "\n",
    "trim_positions = []\n",
    "\n",
    "for i in range(freq_matrix.shape[1]):\n",
    "    if 1 in freq_matrix[:,i]:\n",
    "        trim_positions.append(i)\n",
    "\n",
    "print(trim_positions)\n",
    "\n",
    "\n",
    "vae_alignment_trimmed = []\n",
    "\n",
    "\n",
    "for alignment in vae_alignment:\n",
    "    new_seq = ''\n",
    "    for i in range(62):\n",
    "        if i not in trim_positions:\n",
    "            new_seq+=alignment.seq[i]\n",
    "    re_alignment = SeqRecord(seq=Seq(new_seq), id = alignment.id)\n",
    "    vae_alignment_trimmed.append(re_alignment)\n",
    "\n",
    "vae_alignment_trimmed = AlignIO.MultipleSeqAlignment(vae_alignment_trimmed)\n",
    "\n",
    "AlignIO.write(vae_alignment_trimmed, 'vae_alignment_trimmed.fasta', 'fasta')\n",
    "\n",
    "test_seqs = np.asarray([list(str(alignment.seq)) for alignment in vae_alignment_trimmed])\n",
    "\n",
    "phenotypes = np.asarray(phenotypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c412150d-be7e-438d-a386-16c815e2e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = torch.nn.LayerNorm(normalized_shape)\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        sub_layer_x\n",
    "    ):\n",
    "        add = x + sub_layer_x\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e518096f-e882-4361-a68b-38141628b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveNeuralProcess_Deterministic(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_dim, \n",
    "        y_dim,\n",
    "        projected_dim,\n",
    "        d_hidden, \n",
    "        d_model,\n",
    "        heads,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.linear_1 = torch.nn.Linear(x_dim + y_dim, projected_dim)\n",
    "        self.activation = torch.nn.SELU()\n",
    "\n",
    "        self.self_mha_1 = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "        self.addnorm_1 = AddNorm(normalized_shape= projected_dim)\n",
    "        self.dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "        self.self_mha_2 = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "        self.addnorm_2 = AddNorm(normalized_shape=projected_dim)\n",
    "        self.dropout_2 = torch.nn.Dropout(0.1)\n",
    "\n",
    "        self.context_projection = torch.nn.Linear(x_dim, projected_dim)\n",
    "        self.target_projection = torch.nn.Linear(x_dim, projected_dim)\n",
    "        \n",
    "        self.cross_mha = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "\n",
    "        self.linear_2 = torch.nn.Linear(projected_dim + x_dim, projected_dim)\n",
    "        self.linear_3 = torch.nn.Linear(projected_dim + x_dim, projected_dim)\n",
    "        self.linear_4 = torch.nn.Linear(projected_dim + x_dim, projected_dim)\n",
    "        self.linear_5 = torch.nn.Linear(projected_dim + x_dim, y_dim)\n",
    "\n",
    "    def cross_entropy(self, x, y):\n",
    "        return torch.sum(-y * torch.log(x + 1e-6) - (1.-y)*torch.log(1. - x + 1e-6))\n",
    "\n",
    "    def forward(self, context_x, context_y, target_x, target_y = None):\n",
    "        context = torch.concat([context_x, context_y], dim=-1)\n",
    "        x_1 = self.activation(self.linear_1(context))\n",
    "\n",
    "        x_2, _ = self.self_mha_1(x_1, x_1, x_1)\n",
    "        x_2    = self.addnorm_1(x_2, x_1)\n",
    "        x_2    = self.dropout_1(x_2)\n",
    "\n",
    "        x_3, _ = self.self_mha_2(x_2, x_2, x_2)\n",
    "        x_3    = self.addnorm_2(x_3, x_2)\n",
    "        x_3    = self.dropout_2(x_2)\n",
    "\n",
    "        projected_context = self.context_projection(context_x)\n",
    "        projected_target  = self.target_projection(target_x)\n",
    "        \n",
    "        cross_attended,_ = self.cross_mha(projected_target, projected_context, x_3)\n",
    "        \n",
    "        yhat = torch.concat([cross_attended, x_target], dim=-1)\n",
    "        yhat = self.activation(self.linear_2(yhat))\n",
    "        \n",
    "        yhat = torch.concat([yhat, x_target], dim=-1)\n",
    "        yhat = self.activation(self.linear_3(yhat))\n",
    "        \n",
    "        yhat = torch.concat([yhat, x_target], dim=-1)\n",
    "        yhat = self.activation(self.linear_4(yhat))\n",
    "\n",
    "        yhat = torch.concat([yhat, x_target], dim=-1)\n",
    "        yhat = self.linear_5(yhat)\n",
    "        \n",
    "        yhat = torch.nn.Softmax(dim=-1)(yhat)\n",
    "\n",
    "        if target_y is not None:\n",
    "            cross_entropy = self.cross_entropy(yhat, target_y)\n",
    "            return yhat, cross_entropy\n",
    "        else:\n",
    "            return yhat, 0\n",
    "                                     \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fce7f86a-c0f8-480c-babc-fdddab86ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANP_det = AttentiveNeuralProcess_Deterministic(\n",
    "    1, 21, 128, 128, 128, 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ad5e1d8-3073-4980-8db0-4095b18d0939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentiveNeuralProcess_Deterministic(\n",
       "  (linear_1): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (activation): SELU()\n",
       "  (self_mha_1): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (addnorm_1): AddNorm(\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (self_mha_2): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (addnorm_2): AddNorm(\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "  (context_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (target_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (cross_mha): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear_2): Linear(in_features=129, out_features=128, bias=True)\n",
       "  (linear_3): Linear(in_features=129, out_features=128, bias=True)\n",
       "  (linear_4): Linear(in_features=129, out_features=128, bias=True)\n",
       "  (linear_5): Linear(in_features=129, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANP_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe663393-e00f-4b74-961a-7f6ee0e76c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = ProteinDataset(data=seqs)\n",
    "min_context = int(0.1 * seqs.shape[1])\n",
    "max_context = int(0.9 * seqs.shape[1])\n",
    "len_aa = seqs.shape[1]\n",
    "loader = torch.utils.data.DataLoader(proteins, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch in loader:\n",
    "    break\n",
    "(((x_context, y_context), x_target), y_target) = context_target_splitter(batch, min_context, max_context, len_aa, 1, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f64963f5-03f5-414b-a235-5a630709af01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.4386e-02, 8.0949e-02, 4.7969e-02,  ..., 2.7180e-02,\n",
       "           6.0112e-02, 2.8990e-02],\n",
       "          [1.8848e-05, 4.9971e-02, 3.3127e-02,  ..., 2.7529e-04,\n",
       "           2.9538e-03, 2.7720e-03],\n",
       "          [4.8693e-02, 5.0173e-02, 4.7847e-02,  ..., 4.9217e-02,\n",
       "           5.9219e-02, 4.6083e-02],\n",
       "          ...,\n",
       "          [7.7137e-03, 8.5266e-02, 4.9382e-02,  ..., 1.8439e-02,\n",
       "           5.0621e-02, 2.3808e-02],\n",
       "          [3.3336e-03, 8.5582e-02, 4.9694e-02,  ..., 1.0796e-02,\n",
       "           3.7406e-02, 1.8472e-02],\n",
       "          [9.0443e-03, 8.4513e-02, 4.9091e-02,  ..., 2.0411e-02,\n",
       "           5.3197e-02, 2.5008e-02]],\n",
       " \n",
       "         [[3.9141e-04, 7.2935e-02, 4.3625e-02,  ..., 2.5580e-03,\n",
       "           1.4496e-02, 8.8753e-03],\n",
       "          [1.1099e-04, 6.3532e-02, 3.9039e-02,  ..., 1.0403e-03,\n",
       "           7.6718e-03, 5.5171e-03],\n",
       "          [4.8692e-02, 4.8670e-02, 4.6925e-02,  ..., 4.9639e-02,\n",
       "           6.1355e-02, 4.5813e-02],\n",
       "          ...,\n",
       "          [2.3244e-03, 8.3840e-02, 4.8145e-02,  ..., 8.6566e-03,\n",
       "           3.2804e-02, 1.6396e-02],\n",
       "          [3.2942e-03, 8.4967e-02, 4.8520e-02,  ..., 1.0894e-02,\n",
       "           3.7728e-02, 1.8339e-02],\n",
       "          [5.3501e-06, 4.0995e-02, 2.7662e-02,  ..., 1.0527e-04,\n",
       "           1.4769e-03, 1.6089e-03]],\n",
       " \n",
       "         [[5.5055e-03, 8.5471e-02, 4.9351e-02,  ..., 1.4981e-02,\n",
       "           4.5250e-02, 2.1890e-02],\n",
       "          [1.0514e-02, 8.3391e-02, 4.8458e-02,  ..., 2.2549e-02,\n",
       "           5.5912e-02, 2.6862e-02],\n",
       "          [2.2334e-05, 5.1478e-02, 3.3518e-02,  ..., 3.1670e-04,\n",
       "           3.2321e-03, 2.9915e-03],\n",
       "          ...,\n",
       "          [7.7996e-05, 6.1073e-02, 3.8382e-02,  ..., 8.0326e-04,\n",
       "           6.3229e-03, 4.9257e-03],\n",
       "          [9.3328e-05, 6.2459e-02, 3.9080e-02,  ..., 9.1518e-04,\n",
       "           6.9422e-03, 5.2811e-03],\n",
       "          [7.6778e-06, 4.3726e-02, 2.9511e-02,  ..., 1.3995e-04,\n",
       "           1.7857e-03, 1.9287e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[2.8062e-03, 8.4910e-02, 4.9449e-02,  ..., 9.6835e-03,\n",
       "           3.5035e-02, 1.7214e-02],\n",
       "          [3.1820e-05, 5.4340e-02, 3.4964e-02,  ..., 4.1616e-04,\n",
       "           3.9608e-03, 3.3674e-03],\n",
       "          [3.3469e-02, 6.3375e-02, 4.6379e-02,  ..., 4.3136e-02,\n",
       "           6.6135e-02, 3.8106e-02],\n",
       "          ...,\n",
       "          [5.4463e-05, 5.8538e-02, 3.7057e-02,  ..., 6.2030e-04,\n",
       "           5.2744e-03, 4.1694e-03],\n",
       "          [4.9314e-02, 4.8969e-02, 4.7575e-02,  ..., 4.9518e-02,\n",
       "           6.0377e-02, 4.6454e-02],\n",
       "          [3.3391e-03, 8.5318e-02, 4.9618e-02,  ..., 1.0842e-02,\n",
       "           3.7561e-02, 1.8171e-02]],\n",
       " \n",
       "         [[1.8536e-02, 7.4533e-02, 4.8363e-02,  ..., 3.3388e-02,\n",
       "           6.4826e-02, 3.2787e-02],\n",
       "          [9.2886e-06, 4.5532e-02, 2.9911e-02,  ..., 1.5733e-04,\n",
       "           1.9635e-03, 2.0605e-03],\n",
       "          [3.1961e-06, 3.8247e-02, 2.6130e-02,  ..., 6.8541e-05,\n",
       "           1.0749e-03, 1.3160e-03],\n",
       "          ...,\n",
       "          [9.6798e-04, 7.9998e-02, 4.7298e-02,  ..., 4.7696e-03,\n",
       "           2.2053e-02, 1.2420e-02],\n",
       "          [2.7718e-02, 6.5358e-02, 4.7479e-02,  ..., 4.1694e-02,\n",
       "           6.7512e-02, 3.7825e-02],\n",
       "          [3.2990e-03, 8.5064e-02, 4.9834e-02,  ..., 1.0939e-02,\n",
       "           3.7541e-02, 1.8704e-02]],\n",
       " \n",
       "         [[2.2601e-02, 7.4173e-02, 4.8112e-02,  ..., 3.5068e-02,\n",
       "           6.3500e-02, 3.4220e-02],\n",
       "          [4.9250e-02, 4.9647e-02, 4.7445e-02,  ..., 4.8398e-02,\n",
       "           5.9411e-02, 4.6006e-02],\n",
       "          [3.3671e-02, 6.3998e-02, 4.7655e-02,  ..., 4.2755e-02,\n",
       "           6.4606e-02, 3.8868e-02],\n",
       "          ...,\n",
       "          [2.8099e-03, 8.5350e-02, 5.0161e-02,  ..., 9.6204e-03,\n",
       "           3.3985e-02, 1.7890e-02],\n",
       "          [3.9908e-04, 7.3602e-02, 4.5183e-02,  ..., 2.5348e-03,\n",
       "           1.4084e-02, 9.1827e-03],\n",
       "          [7.7713e-03, 8.5415e-02, 5.0312e-02,  ..., 1.8433e-02,\n",
       "           4.9087e-02, 2.4395e-02]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor(10081.3184, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANP_det(x_context, y_context, x_target, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "725298f2-5844-45a0-84f5-0a0a5d9e3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, step_num, warmup_step=4000):\n",
    "    lr = 0.001 * warmup_step**0.5 * min(step_num * warmup_step**-1.5, step_num**-0.5)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95ece645-791d-4f8c-9867-549010b2e569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss at epoch 0 is 3.8398653389166606\n",
      "validation loss at epoch 1 is 3.6818944282711312\n",
      "validation loss at epoch 2 is 3.675802741242159\n",
      "validation loss at epoch 3 is 3.6339111435067886\n",
      "validation loss at epoch 4 is 3.6333259163927254\n",
      "validation loss at epoch 5 is 3.5727878121351964\n",
      "validation loss at epoch 6 is 3.6348869345921115\n",
      "validation loss at epoch 7 is 3.543925056804638\n",
      "validation loss at epoch 8 is 3.4805710358787847\n",
      "validation loss at epoch 9 is 3.5622284862741687\n",
      "validation loss at epoch 10 is 3.5153081674442905\n",
      "validation loss at epoch 11 is 3.551105006038081\n",
      "validation loss at epoch 12 is 3.4129231362073864\n",
      "validation loss at epoch 13 is 3.360386997668451\n",
      "validation loss at epoch 14 is 3.310965486525219\n",
      "validation loss at epoch 15 is 3.2961232692306464\n",
      "validation loss at epoch 16 is 3.224149542696254\n",
      "validation loss at epoch 17 is 3.2213383350796296\n",
      "validation loss at epoch 18 is 3.1994921566148946\n",
      "validation loss at epoch 19 is 3.2292350682864828\n",
      "validation loss at epoch 20 is 3.211406668417602\n",
      "validation loss at epoch 21 is 3.2028098378921537\n",
      "validation loss at epoch 22 is 3.171502060929132\n",
      "validation loss at epoch 23 is 3.1210378302784134\n",
      "validation loss at epoch 24 is 3.1128915779324\n",
      "validation loss at epoch 25 is 3.0948261327424595\n",
      "validation loss at epoch 26 is 3.086296153640297\n",
      "validation loss at epoch 27 is 3.085772453619755\n",
      "validation loss at epoch 28 is 3.0828762841587554\n",
      "validation loss at epoch 29 is 3.0651292065113993\n",
      "validation loss at epoch 30 is 3.0251948361050953\n",
      "validation loss at epoch 31 is 3.022525481095829\n",
      "validation loss at epoch 32 is 3.0002557966865067\n",
      "validation loss at epoch 33 is 2.981150019310748\n",
      "validation loss at epoch 34 is 2.997368055750746\n",
      "validation loss at epoch 35 is 3.058898149375905\n",
      "validation loss at epoch 36 is 2.9677373727944327\n",
      "validation loss at epoch 37 is 2.947489713477596\n",
      "validation loss at epoch 38 is 2.9390343744490304\n",
      "validation loss at epoch 39 is 2.9519958364197527\n",
      "validation loss at epoch 40 is 2.917731078706587\n",
      "validation loss at epoch 41 is 2.9069306119684817\n",
      "validation loss at epoch 42 is 2.9141407240861477\n",
      "validation loss at epoch 43 is 2.889651170960139\n",
      "validation loss at epoch 44 is 2.860430829279276\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "proteins = ProteinDataset(data=seqs)\n",
    "syn_proteins = ProteinDataset(data=test_seqs)\n",
    "\n",
    "EPOCHS=45\n",
    "ANP_det.train()\n",
    "optim = torch.optim.Adam(ANP_det.parameters(), lr = 1e-3)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "min_context = int(0.1 * seqs.shape[1])\n",
    "max_context = int(0.9 * seqs.shape[1])\n",
    "len_aa = seqs.shape[1]\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loader = torch.utils.data.DataLoader(proteins, batch_size=32, shuffle=True)\n",
    "    overall_loss = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        \n",
    "        global_step+=1\n",
    "        (((x_context, y_context), x_target), y_target) = context_target_splitter(batch, min_context, max_context, len_aa, 1, 21)\n",
    "        \n",
    "        adjust_learning_rate(optim, global_step)\n",
    "        \n",
    "        y_pred, loss = ANP_det(x_context, y_context, x_target, y_target)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        writer.add_scalars('training_loss',{\n",
    "                    'loss':loss,\n",
    "                }, global_step)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(syn_proteins, batch_size=32, shuffle=False)\n",
    "    for batch in val_loader:\n",
    "        \n",
    "        (((x_context, y_context), x_target), y_target) = context_target_splitter(batch, min_context, max_context, len_aa, 1, 21)\n",
    "        val_y_pred, val_loss = ANP_det(x_context, y_context, x_target, y_target)\n",
    "        overall_loss += val_loss.item()\n",
    "\n",
    "    print('validation loss at epoch {} is '.format(epoch) + str(overall_loss/(test_seqs.shape[0]*test_seqs.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c57534f-1b51-481a-8cee-062a00576ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.9829e-01, 5.2741e-03, 6.8949e-04, 8.5269e-05, 2.7267e-02, 9.5098e-03,\n",
       "        1.0855e-02, 1.7403e-02, 5.7025e-03, 1.3441e-02, 1.4761e-02, 6.2775e-03,\n",
       "        1.1058e-02, 6.1907e-03, 9.1694e-04, 6.6062e-03, 8.8631e-03, 4.5381e-03,\n",
       "        6.0706e-02, 1.9154e-01, 1.9941e-05], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y_pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47f0824e-2277-453e-a22e-f7d1423d3cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x149a67ed0>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_target[0,7])\n",
    "plt.plot(val_y_pred.detach().numpy()[0,7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5227a892-1715-4e06-8755-7bb227835614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 59, 1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c3b23b7e-5e9e-4d24-9841-d102ad529566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveNeuralProcess_Latent(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_dim, \n",
    "        y_dim,\n",
    "        projected_dim,\n",
    "        d_hidden, \n",
    "        d_model,\n",
    "        heads,\n",
    "        latent_dim,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.linear_1 = torch.nn.Linear(x_dim + y_dim, projected_dim)\n",
    "        self.activation = torch.nn.SELU()\n",
    "\n",
    "        self.deterministic_self_mha_1 = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "        self.addnorm_1 = AddNorm(normalized_shape= projected_dim)\n",
    "        self.dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "        self.deterministic_self_mha_2 = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "        self.addnorm_2 = AddNorm(normalized_shape=projected_dim)\n",
    "        self.dropout_2 = torch.nn.Dropout(0.1)\n",
    "\n",
    "        self.latent_linear_1 = torch.nn.Linear(x_dim + y_dim, projected_dim)\n",
    "\n",
    "        self.latent_self_mha_1 = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "        self.latent_addnorm_1 = AddNorm(normalized_shape= projected_dim)\n",
    "        self.latent_dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "        self.latent_self_mha_2 = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "        self.latent_addnorm_2 = AddNorm(normalized_shape=projected_dim)\n",
    "        self.latent_dropout_2 = torch.nn.Dropout(0.1)\n",
    "\n",
    "        self.latent_mu = torch.nn.Linear(projected_dim, latent_dim)\n",
    "        self.latent_log_var = torch.nn.Linear(projected_dim, latent_dim)\n",
    "\n",
    "    \n",
    "        self.context_projection = torch.nn.Linear(x_dim, projected_dim)\n",
    "        self.target_projection = torch.nn.Linear(x_dim, projected_dim)\n",
    "        \n",
    "        self.cross_mha = MultiHeadedAttention(heads, projected_dim, projected_dim, projected_dim, d_hidden, projected_dim)\n",
    "\n",
    "        self.linear_2 = torch.nn.Linear(projected_dim + x_dim + latent_dim, projected_dim)\n",
    "        self.linear_3 = torch.nn.Linear(projected_dim + x_dim + latent_dim, projected_dim)\n",
    "        self.linear_4 = torch.nn.Linear(projected_dim + x_dim + latent_dim, projected_dim)\n",
    "        self.linear_5 = torch.nn.Linear(projected_dim + x_dim + latent_dim, y_dim)\n",
    "\n",
    "    def cross_entropy(self, x, y):\n",
    "        return torch.sum(-y * torch.log(x + 1e-6) - (1.-y)*torch.log(1. - x + 1e-6))\n",
    "\n",
    "    def reparam(self, z_mu, z_log_var):\n",
    "        return z_mu + torch.randn_like(z_log_var) * torch.exp(z_log_var/2.)\n",
    "\n",
    "    def kl_loss_calc(\n",
    "        self, z_mu_context, z_log_var_context,\n",
    "              z_mu_target,  z_log_var_target\n",
    "    ):\n",
    "        q_context = torch.distributions.Normal(z_mu_context, torch.exp(z_log_var_context/2))\n",
    "        q_target = torch.distributions.Normal(z_mu_target, torch.exp(z_log_var_target/2))\n",
    "        \n",
    "        kl = torch.distributions.kl.kl_divergence(q_context, q_target)\n",
    "        return kl\n",
    "\n",
    "    def forward(self, context_x, context_y, target_x, target_y = None):\n",
    "        context = torch.concat([context_x, context_y], dim=-1)\n",
    "        x_1 = self.activation(self.linear_1(context))\n",
    "\n",
    "        x_2, _ = self.deterministic_self_mha_1(x_1, x_1, x_1)\n",
    "        x_2    = self.addnorm_1(x_2, x_1)\n",
    "        x_2    = self.dropout_1(x_2)\n",
    "\n",
    "        x_3, _ = self.deterministic_self_mha_2(x_2, x_2, x_2)\n",
    "        x_3    = self.addnorm_2(x_3, x_2)\n",
    "        x_3    = self.dropout_2(x_2)\n",
    "\n",
    "        projected_context = self.context_projection(context_x)\n",
    "        projected_target  = self.target_projection(target_x)\n",
    "        \n",
    "        cross_attended,_ = self.cross_mha(projected_target, projected_context, x_3)\n",
    "\n",
    "        z_1 = self.latent_linear_1(context)\n",
    "\n",
    "        z_2, _ = self.latent_self_mha_1(z_1, z_1, z_1)\n",
    "        z_2    = self.latent_addnorm_1(z_2, z_1)\n",
    "        z_2    = self.latent_dropout_1(z_2)\n",
    "\n",
    "        z_3, _ = self.latent_self_mha_2(z_2, z_2, z_2)\n",
    "        z_3    = self.latent_addnorm_2(z_3, z_2)\n",
    "        z_3    = self.latent_dropout_2(z_3)\n",
    "\n",
    "        z_3 = torch.mean(z_3, dim=1)\n",
    "\n",
    "        z_mu = self.latent_mu(z_3)\n",
    "        z_log_var = self.latent_log_var(z_3)\n",
    "\n",
    "        z = self.reparam(z_mu, z_log_var)\n",
    "\n",
    "        z = z.unsqueeze(dim=1).tile([1, target_x.shape[1],1])\n",
    "\n",
    "        yhat = torch.concat([cross_attended, target_x, z], dim=-1)\n",
    "        yhat = self.activation(self.linear_2(yhat))\n",
    "\n",
    "        yhat = torch.concat([yhat, target_x, z], dim=-1)\n",
    "        yhat = self.activation(self.linear_3(yhat))\n",
    "\n",
    "        yhat = torch.concat([yhat, target_x, z], dim=-1)\n",
    "        yhat = self.activation(self.linear_4(yhat))\n",
    "\n",
    "        yhat = torch.concat([yhat, target_x, z], dim=-1)\n",
    "        yhat = self.linear_5(yhat)\n",
    "        yhat = torch.nn.Softmax(dim=-1)(yhat)\n",
    "\n",
    "        if target_y is not None:\n",
    "            cross_entropy = self.cross_entropy(yhat, target_y)\n",
    "            target = torch.concat([target_x, target_y], dim=-1)\n",
    "            target_1 = self.activation(self.latent_linear_1(target))\n",
    "\n",
    "            target_2, _ = self.latent_self_mha_1(target_1, target_1, target_1)\n",
    "            target_2    = self.latent_addnorm_1(target_2, target_1)\n",
    "            target_2    = self.latent_dropout_1(target_2)\n",
    "\n",
    "            target_3, _ = self.latent_self_mha_2(target_2, target_2, target_2)\n",
    "            target_3    = self.latent_addnorm_2(target_3, target_2)\n",
    "            target_3    = self.latent_dropout_2(target_3)\n",
    "\n",
    "            z_target    = torch.mean(target_3, dim=1)\n",
    "            target_z_mu = self.latent_mu(z_target)\n",
    "            target_z_log_var = self.latent_log_var(z_target)\n",
    "\n",
    "            kl_loss = torch.sum(self.kl_loss_calc(z_mu, z_log_var, target_z_mu, target_z_log_var))\n",
    "            \n",
    "            return yhat, cross_entropy + kl_loss\n",
    "        else:\n",
    "            return yhat, 0\n",
    "                                     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "97b4fdd8-2e37-4e8e-ba6c-89f8e223355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANP_lat = AttentiveNeuralProcess_Latent(\n",
    "    1, 21, 128, 128, 128, 8, 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "12dd3f3e-7c8e-4049-98fa-28738551ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentiveNeuralProcess_Latent(\n",
       "  (linear_1): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (activation): SELU()\n",
       "  (deterministic_self_mha_1): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (addnorm_1): AddNorm(\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (deterministic_self_mha_2): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (addnorm_2): AddNorm(\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "  (latent_linear_1): Linear(in_features=22, out_features=128, bias=True)\n",
       "  (latent_self_mha_1): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (latent_addnorm_1): AddNorm(\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (latent_dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (latent_self_mha_2): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (latent_addnorm_2): AddNorm(\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (latent_dropout_2): Dropout(p=0.1, inplace=False)\n",
       "  (latent_mu): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (latent_log_var): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (context_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (target_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (cross_mha): MultiHeadedAttention(\n",
       "    (attention): DotProductAttention()\n",
       "    (W_q): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_k): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_v): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (W_o): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear_2): Linear(in_features=133, out_features=128, bias=True)\n",
       "  (linear_3): Linear(in_features=133, out_features=128, bias=True)\n",
       "  (linear_4): Linear(in_features=133, out_features=128, bias=True)\n",
       "  (linear_5): Linear(in_features=133, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANP_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057219f-0f13-4215-bdbe-f75c7d6c907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246it [02:43,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss at epoch 0 is 3.8207671812214543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246it [02:49,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss at epoch 1 is 3.7047326445842708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246it [03:47,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss at epoch 2 is 3.704501546713519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [02:13,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "proteins = ProteinDataset(data=seqs)\n",
    "syn_proteins = ProteinDataset(data=test_seqs)\n",
    "\n",
    "EPOCHS=45\n",
    "ANP_lat.train()\n",
    "optim = torch.optim.Adam(ANP_lat.parameters(), lr = 1e-3)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "min_context = int(0.1 * seqs.shape[1])\n",
    "max_context = int(0.9 * seqs.shape[1])\n",
    "len_aa = seqs.shape[1]\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loader = torch.utils.data.DataLoader(proteins, batch_size=100, shuffle=True)\n",
    "    overall_loss = 0\n",
    "    for i, batch in tqdm(enumerate(loader)):\n",
    "\n",
    "        \n",
    "        global_step+=1\n",
    "        (((x_context, y_context), x_target), y_target) = context_target_splitter(batch, min_context, max_context, len_aa, 1, 21)\n",
    "        \n",
    "        adjust_learning_rate(optim, global_step)\n",
    "        \n",
    "        y_pred, loss = ANP_lat(x_context, y_context, x_target, y_target)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        writer.add_scalars('training_loss',{\n",
    "                    'loss':loss,\n",
    "                }, global_step)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(syn_proteins, batch_size=100, shuffle=False)\n",
    "    for batch in val_loader:\n",
    "        \n",
    "        (((x_context, y_context), x_target), y_target) = context_target_splitter(batch, min_context, max_context, len_aa, 1, 21)\n",
    "        val_y_pred, val_loss = ANP_lat(x_context, y_context, x_target, y_target)\n",
    "        overall_loss += val_loss.item()\n",
    "\n",
    "    print('validation loss at epoch {} is '.format(epoch) + str(overall_loss/(test_seqs.shape[0]*test_seqs.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4fc72eea-afd1-4642-a6ef-6678a6f5d0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0725, -0.5524, -0.5552]],\n",
       "\n",
       "        [[-0.8216,  0.6530, -0.0530]],\n",
       "\n",
       "        [[ 1.7631, -1.6940,  0.9103]],\n",
       "\n",
       "        [[ 2.1919, -0.0126,  0.0225]],\n",
       "\n",
       "        [[-0.6425, -1.7878,  2.2622]],\n",
       "\n",
       "        [[ 1.0786,  0.0765, -1.6339]],\n",
       "\n",
       "        [[-0.0836,  0.1963,  0.6273]],\n",
       "\n",
       "        [[ 0.7639,  0.3908,  1.4168]],\n",
       "\n",
       "        [[ 0.0147,  0.4423,  0.0785]],\n",
       "\n",
       "        [[ 1.1422,  1.0022, -0.1402]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15edf906-f55c-4ef0-b91e-3f2d9e7ad6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_test = torch.randn((10,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c5bec55-bebd-48f7-a523-057995c41dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.7885e-01,  1.8503e+00,  4.2787e-01],\n",
       "         [ 1.0071e+00,  1.4661e+00, -1.0260e-01],\n",
       "         [-1.3728e+00,  1.9242e+00, -6.2077e-01],\n",
       "         [-6.5727e-01, -7.5758e-01,  5.4797e-01],\n",
       "         [-1.0862e+00,  1.4214e+00, -1.4982e-01]],\n",
       "\n",
       "        [[-7.6549e-01,  1.5078e-01,  2.9107e-04],\n",
       "         [-2.7137e-01, -1.8251e+00,  1.6859e+00],\n",
       "         [-2.1129e+00,  2.3690e+00,  8.0102e-01],\n",
       "         [ 2.6335e-02,  1.9552e-01, -4.6229e-01],\n",
       "         [-2.7230e+00, -1.3442e+00, -8.8763e-01]],\n",
       "\n",
       "        [[ 5.1638e-01,  7.2710e-01, -1.8898e+00],\n",
       "         [-3.2572e-01,  1.0587e+00, -9.0792e-01],\n",
       "         [-1.1495e+00,  4.6414e-01, -4.8468e-01],\n",
       "         [ 1.0646e+00,  4.4712e-01, -6.7189e-01],\n",
       "         [-9.7373e-01, -3.1889e-01, -1.9782e+00]],\n",
       "\n",
       "        [[ 1.4106e+00,  8.3720e-01,  2.1635e-01],\n",
       "         [-1.3530e+00,  2.7244e-01,  8.5228e-01],\n",
       "         [-2.9217e+00,  4.1772e-03,  9.2262e-01],\n",
       "         [ 1.2403e+00,  5.4462e-01, -1.6630e-01],\n",
       "         [-5.1164e-01,  1.2458e+00,  9.4213e-01]],\n",
       "\n",
       "        [[ 1.1896e+00,  1.0729e+00,  6.9343e-01],\n",
       "         [ 6.4366e-01,  8.5062e-01,  2.5932e-02],\n",
       "         [-7.4051e-01, -1.4823e+00, -1.8003e+00],\n",
       "         [ 4.7036e-01,  3.3106e-01,  1.3796e+00],\n",
       "         [ 3.8345e-01, -4.8187e-01,  9.6876e-01]],\n",
       "\n",
       "        [[-8.6573e-01, -9.9244e-02,  1.7295e-01],\n",
       "         [ 1.2011e+00, -1.9013e+00,  1.0202e-01],\n",
       "         [-6.1840e-01, -1.3834e+00,  3.3889e-01],\n",
       "         [-2.4261e-01, -3.6381e-01, -3.7161e-01],\n",
       "         [-2.9350e-01,  1.4247e+00,  1.7190e-01]],\n",
       "\n",
       "        [[-7.2174e-01,  2.1935e+00,  2.4837e+00],\n",
       "         [-1.2010e+00,  1.1538e+00,  1.2728e+00],\n",
       "         [ 2.3710e+00,  9.7967e-01,  4.2706e-01],\n",
       "         [ 3.7177e-01,  1.1626e+00,  6.1843e-02],\n",
       "         [-1.1670e-01, -2.0090e-02,  2.9228e-01]],\n",
       "\n",
       "        [[-1.1489e+00,  8.3289e-01, -4.0409e-01],\n",
       "         [-6.8746e-02,  1.5216e+00,  2.7866e-01],\n",
       "         [ 1.2541e+00,  4.2922e-01, -3.7377e+00],\n",
       "         [ 6.9316e-01, -8.0008e-01,  1.0356e+00],\n",
       "         [-1.8558e-01,  1.3026e+00, -1.1576e+00]],\n",
       "\n",
       "        [[-1.7209e-01,  4.9787e-01, -2.8419e-01],\n",
       "         [-1.5587e+00, -4.2459e-01,  9.7269e-03],\n",
       "         [ 2.5824e-01,  9.5018e-01, -1.5500e-01],\n",
       "         [ 4.5351e-01,  6.5553e-02,  8.9437e-01],\n",
       "         [ 6.8949e-02, -6.4023e-01,  6.9652e-01]],\n",
       "\n",
       "        [[ 1.4921e-01,  3.0085e-02, -2.6528e-02],\n",
       "         [-1.1841e+00,  1.2183e+00, -1.0966e+00],\n",
       "         [ 6.7315e-01,  3.3394e-01, -1.2807e+00],\n",
       "         [-1.5279e-01, -1.1339e+00,  9.1868e-01],\n",
       "         [-5.2404e-01,  1.5888e+00, -1.0171e+00]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "026fcbf3-620c-4cc1-b49d-1783dd84e573",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 1 but got size 5 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 5 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "torch.concat([test, other_test], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248da43-2e65-45e8-846f-98d5420800c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
